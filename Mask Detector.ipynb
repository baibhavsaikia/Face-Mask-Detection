{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9051aa93",
   "metadata": {},
   "source": [
    "# COVID-19 Face Mask Detection\n",
    "### Detects when a user is not wearing a mask and displays it. \n",
    "\n",
    "Done by- Baibhav Saikia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c895e",
   "metadata": {},
   "source": [
    "### Importing the Dataset and Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b91720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539793c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with mask': 0, 'without mask': 1}\n"
     ]
    }
   ],
   "source": [
    "data_path=r\"C:\\Users\\asus\\Desktop\\Dataset\"\n",
    "folders=os.listdir(data_path)\n",
    "labels=[i for i in range(len(folders))]\n",
    "label_dict=dict(zip(folders,labels))\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20742d6f",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0004c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=100\n",
    "data=[]\n",
    "label=[]\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path=os.path.join(data_path,folder)\n",
    "    img_names=os.listdir(folder_path)\n",
    "        \n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "        gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)           \n",
    "        resized=cv2.resize(gray,(img_size,img_size))\n",
    "        data.append(resized)\n",
    "        label.append(label_dict[folder])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ee9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(data)/255.0\n",
    "data=np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "label=np.array(label)\n",
    "label=np_utils.to_categorical(label)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b73d7",
   "metadata": {},
   "source": [
    "### Making the Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725aa0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(200,(3,3),input_shape=data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(100,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882ff22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_label,test_label=train_test_split(data,label,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78943efd",
   "metadata": {},
   "source": [
    "### Training and Testing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e991cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "31/31 [==============================] - 82s 2s/step - loss: 0.7477 - accuracy: 0.5048 - val_loss: 0.6537 - val_accuracy: 0.7339\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "Epoch 2/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.6508 - accuracy: 0.6246 - val_loss: 0.4896 - val_accuracy: 0.8024\n",
      "INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "Epoch 3/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.4445 - accuracy: 0.8113 - val_loss: 0.2744 - val_accuracy: 0.8992\n",
      "INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "Epoch 4/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.2766 - accuracy: 0.9048 - val_loss: 0.2224 - val_accuracy: 0.9194\n",
      "INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "Epoch 5/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.2028 - accuracy: 0.9181 - val_loss: 0.1751 - val_accuracy: 0.9274\n",
      "INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "Epoch 6/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.1397 - accuracy: 0.9519 - val_loss: 0.1270 - val_accuracy: 0.9516\n",
      "INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "Epoch 7/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.1248 - accuracy: 0.9590 - val_loss: 0.1185 - val_accuracy: 0.9597\n",
      "INFO:tensorflow:Assets written to: model-007.model\\assets\n",
      "Epoch 8/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.1003 - accuracy: 0.9661 - val_loss: 0.1585 - val_accuracy: 0.9395\n",
      "Epoch 9/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.1266 - accuracy: 0.9490 - val_loss: 0.0864 - val_accuracy: 0.9677\n",
      "INFO:tensorflow:Assets written to: model-009.model\\assets\n",
      "Epoch 10/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.0613 - accuracy: 0.9754 - val_loss: 0.0774 - val_accuracy: 0.9758\n",
      "INFO:tensorflow:Assets written to: model-010.model\\assets\n",
      "Epoch 11/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.0470 - accuracy: 0.9820 - val_loss: 0.0863 - val_accuracy: 0.9758\n",
      "Epoch 12/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.0492 - accuracy: 0.9839 - val_loss: 0.1216 - val_accuracy: 0.9556\n",
      "Epoch 13/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.0424 - accuracy: 0.9840 - val_loss: 0.0686 - val_accuracy: 0.9718\n",
      "INFO:tensorflow:Assets written to: model-013.model\\assets\n",
      "Epoch 14/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.0313 - accuracy: 0.9904 - val_loss: 0.0694 - val_accuracy: 0.9798\n",
      "Epoch 15/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.0259 - accuracy: 0.9961 - val_loss: 0.0760 - val_accuracy: 0.9718\n",
      "Epoch 16/25\n",
      "31/31 [==============================] - 55s 2s/step - loss: 0.0232 - accuracy: 0.9903 - val_loss: 0.0911 - val_accuracy: 0.9798\n",
      "Epoch 17/25\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0546 - accuracy: 0.9823 - val_loss: 0.1364 - val_accuracy: 0.9556\n",
      "Epoch 18/25\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.0464 - accuracy: 0.9818 - val_loss: 0.0678 - val_accuracy: 0.9758\n",
      "INFO:tensorflow:Assets written to: model-018.model\\assets\n",
      "Epoch 19/25\n",
      "31/31 [==============================] - 193s 6s/step - loss: 0.0214 - accuracy: 0.9927 - val_loss: 0.0865 - val_accuracy: 0.9798\n",
      "Epoch 20/25\n",
      "31/31 [==============================] - 56s 2s/step - loss: 0.0197 - accuracy: 0.9932 - val_loss: 0.0825 - val_accuracy: 0.9758\n",
      "Epoch 21/25\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.0381 - accuracy: 0.9862 - val_loss: 0.3536 - val_accuracy: 0.8669\n",
      "Epoch 22/25\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.0990 - accuracy: 0.9688 - val_loss: 0.1239 - val_accuracy: 0.9637\n",
      "Epoch 23/25\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.1076 - accuracy: 0.9530 - val_loss: 0.0916 - val_accuracy: 0.9637\n",
      "Epoch 24/25\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.0218 - accuracy: 0.9943 - val_loss: 0.0674 - val_accuracy: 0.9758\n",
      "INFO:tensorflow:Assets written to: model-024.model\\assets\n",
      "Epoch 25/25\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0087 - accuracy: 0.9984 - val_loss: 0.1204 - val_accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "history=model.fit(train_data,train_label,epochs=25,callbacks=[checkpoint],validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84dc1ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 381ms/step - loss: 0.1746 - accuracy: 0.9783\n",
      "[0.1745682656764984, 0.97826087474823]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(test_data,test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3245ea",
   "metadata": {},
   "source": [
    "### Loading the 'Best' Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30d9ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model-024.model')\n",
    "classifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "source=cv2.VideoCapture(0)\n",
    "labels_dict={1:'NO MASK'}\n",
    "color_dict={1:(0,0,255)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600c075",
   "metadata": {},
   "source": [
    "### Reading from the Input Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc989a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "\n",
    "    ret,img=source.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=classifier.detectMultiScale(gray,1.3,5)  \n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "    \n",
    "        face_img=gray[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,100,100,1))\n",
    "        result=model.predict(reshaped)\n",
    "        lab=np.argmax(result)\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[lab],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[lab],-1)\n",
    "        cv2.putText(img, labels_dict[lab], (x, y-10),cv2.FONT_HERSHEY_DUPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    cv2.imshow('MASK DETECTOR',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    if(key==27):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "source.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f6054",
   "metadata": {},
   "source": [
    "## Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
